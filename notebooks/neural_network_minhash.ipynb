{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9a4c80",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Understanding the selectivity of kmers in phages, against bacteria targets\\\n",
    "Each phage has a confirmed binding to a bacteria. The kmers of the phage can therefore be denoted with True/False for binding to a bacteria.\\\n",
    "Using MinHash sketches, the kmers are compressed, while its uniqueness is still preserved.\\\n",
    "With MinHashed kmers however, we cant \"go back\" and extract which kmers were most important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d5b1f",
   "metadata": {},
   "source": [
    "## Prepping data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ef3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sourmash, os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "K = 12 #kmer size in nucleotides\n",
    "raw_data_path = \"../raw_data/\"\n",
    "data_prod_path = \"../data_prod/\"\n",
    "SKETCH_DIR = data_prod_path + f\"phage_minhash_{K}/\"\n",
    "\n",
    "def binarize_host_range(host_range_dict):\n",
    "    binary_dict = {}\n",
    "    for host, val in host_range_dict.items():\n",
    "        if pd.isna(val) or val == 0:\n",
    "            binary_dict[host] = 0\n",
    "        else:\n",
    "            binary_dict[host] = 1\n",
    "    return binary_dict\n",
    "\n",
    "def short_species_name(full_name):\n",
    "    if len(full_name.split(\" \")) < 2:\n",
    "        return full_name\n",
    "    else:\n",
    "        return full_name.split(\" \")[0][0] + \".\" + full_name.split(\" \")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc2df2",
   "metadata": {},
   "source": [
    "### Phage kmer data - MinHashed sketches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c93c31",
   "metadata": {},
   "source": [
    "Prepping hostrange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041cc9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacteria lookup dictionary created with 110 entries.\n",
      "{'J14_21_reoriented': 'Acinetobacter calcoaceticus', 'J53_21_reoriented': 'Acinetobacter calcoaceticus', 'J105_22_reoriented': 'Chishuiella', 'J46_21_reoriented': 'Chryseobacterium', 'J50_21_reoriented': 'Chryseobacterium', 'J2264_1_22_KMC_reoriented': 'Chryseobacterium', 'J2264_3_22_KMC_reoriented': 'Chryseobacterium', 'J63_22_reoriented': 'Chryseobacterium', 'J64_22_reoriented': 'Chryseobacterium', 'J1_21_reoriented': 'Lelliottia', 'J91_22_reoriented': 'Lelliottia', 'J51_21_reoriented': 'Morganella morganii', 'J57_21_reoriented': 'Morganella morganii', 'J10_21_reoriented': 'Pectobacterium atrosepticum', 'J11_21_reoriented': 'Pectobacterium atrosepticum', 'J126_23_reoriented': 'Pectobacterium atrosepticum', 'J12_21_reoriented': 'Pectobacterium atrosepticum', 'J16_21_reoriented': 'Pectobacterium atrosepticum', 'J22_21_reoriented': 'Pectobacterium atrosepticum', 'J28_21_reoriented': 'Pectobacterium atrosepticum', 'J33_21_reoriented': 'Pectobacterium atrosepticum', 'J38_21_reoriented': 'Pectobacterium atrosepticum', 'J39_21_reoriented': 'Pectobacterium atrosepticum', 'J40_21_reoriented': 'Pectobacterium atrosepticum', 'J56_21_reoriented': 'Pectobacterium atrosepticum', 'J6_21_reoriented': 'Pectobacterium atrosepticum', 'J7_21_reoriented': 'Pectobacterium atrosepticum', 'J8_21_reoriented': 'Pectobacterium atrosepticum', 'J116_23_reoriented': 'Pectobacterium atrosepticum', 'J118_23_reoriented': 'Pectobacterium atrosepticum', 'J121_23_reoriented': 'Pectobacterium atrosepticum', 'J125_23_reoriented': 'Pectobacterium atrosepticum', 'J70_21_reoriented': 'Pectobacterium atrosepticum', 'J78_22_reoriented': 'Pectobacterium atrosepticum', 'J81_22_reoriented': 'Pectobacterium atrosepticum', 'J82_22_reoriented': 'Pectobacterium atrosepticum', 'J84_22_reoriented': 'Pectobacterium atrosepticum', 'J119_23_reoriented': 'Pectobacterium brasiliense', 'J127_23_reoriented': 'Pectobacterium brasiliense', 'J13_21_reoriented': 'Pectobacterium brasiliense', 'J20_21_reoriented': 'Pectobacterium brasiliense', 'J21_21_reoriented': 'Pectobacterium brasiliense', 'J24_21_reoriented': 'Pectobacterium brasiliense', 'J25_21_reoriented': 'Pectobacterium brasiliense', 'J26_21_reoriented': 'Pectobacterium brasiliense', 'J31_21_reoriented': 'Pectobacterium brasiliense', 'J32_21_reoriented': 'Pectobacterium brasiliense', 'J34_21_reoriented': 'Pectobacterium brasiliense', 'J36_21_reoriented': 'Pectobacterium brasiliense', 'J37_21_reoriented': 'Pectobacterium brasiliense', 'J41_21_reoriented': 'Pectobacterium brasiliense', 'J44_21_reoriented': 'Pectobacterium brasiliense', 'J45_21_reoriented': 'Pectobacterium brasiliense', 'J47_21_reoriented': 'Pectobacterium brasiliense', 'J52_21_reoriented': 'Pectobacterium brasiliense', 'J59_21_reoriented': 'Pectobacterium brasiliense', 'J115_23_reoriented': 'Pectobacterium brasiliense', 'J117_23_reoriented': 'Pectobacterium brasiliense', 'J128_23_reoriented': 'Pectobacterium brasiliense', 'J134_23_reoriented': 'Pectobacterium brasiliense', 'J3009LEM_22_KMC_reoriented': 'Pectobacterium brasiliense', 'J3009_2_22_KMC_reoriented': 'Pectobacterium brasiliense', 'J69_21_reoriented': 'Pectobacterium brasiliense', 'J73_21_reoriented': 'Pectobacterium brasiliense', 'J77_22_reoriented': 'Pectobacterium brasiliense', 'J86_22_reoriented': 'Pectobacterium brasiliense', 'J97_22_reoriented': 'Pectobacterium brasiliense', 'J98_22_reoriented': 'Pectobacterium brasiliense', 'J54_21_reoriented': 'Pectobacterium carotovorum', 'J100_22_reoriented': 'Pectobacterium carotovorum', 'J102_22_reoriented': 'Pectobacterium carotovorum', 'J74_21_reoriented': 'Pectobacterium carotovorum', 'J80_22_reoriented': 'Pectobacterium carotovorum', 'J27_21_reoriented': 'Pectobacterium parmentieri', 'J29_21_reoriented': 'Pectobacterium parmentieri', 'J30_21_reoriented': 'Pectobacterium parmentieri', 'J42_21_reoriented': 'Pectobacterium parmentieri', 'J49_21_reoriented': 'Pectobacterium parmentieri', 'FO3A_23_KMC_reoriented': 'Pectobacterium parmentieri', 'J103_22_reoriented': 'Pectobacterium parmentieri', 'J104_22_reoriented': 'Pectobacterium parmentieri', 'J106_22_reoriented': 'Pectobacterium parmentieri', 'J108_22_reoriented': 'Pectobacterium parmentieri', 'J62_22_reoriented': 'Pectobacterium parmentieri', 'J65_21_reoriented': 'Pectobacterium parmentieri', 'J68_21_reoriented': 'Pectobacterium parmentieri', 'J83_22_reoriented': 'Pectobacterium parmentieri', 'J93_22_reoriented': 'Pectobacterium parmentieri', 'J95_22_reoriented': 'Pectobacterium parmentieri', 'J96_22_reoriented': 'Pectobacterium parmentieri', 'J17_21_reoriented': 'Pectobacterium polaris', 'J18_21_reoriented': 'Pectobacterium polaris', 'J19_21_reoriented': 'Pectobacterium polaris', 'J48_21_reoriented': 'Pectobacterium polaris', 'J3009_1_22_KMC_reoriented': 'Pectobacterium polaris', 'J60_21_reoriented': 'Pectobacterium polaris', 'J61_21_reoriented': 'Pectobacterium polaris', 'J66_21_reoriented': 'Pectobacterium polaris', 'J75_21_reoriented': 'Pectobacterium polaris', 'J76_21_reoriented': 'Pectobacterium polaris', 'J79_22_reoriented': 'Pectobacterium polaris', 'J94_22_reoriented': 'Pectobacterium polaris', 'J99_22_reoriented': 'Pectobacterium polaris', 'J2_21_reoriented': 'Pectobacterium punjabense', 'J3_21_reoriented': 'Pectobacterium punjabense', 'J109_23_reoriented': 'Pseudomonas chlororaphis', 'J101_22_reoriented': 'Pseudomonas marginalis', 'J15_21_reoriented': 'Serratia liquefaciens', 'J4_21_reoriented': 'Serratia plymuthica', 'J5_21_reoriented': 'Vagococcus'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phage</th>\n",
       "      <th>Ymer</th>\n",
       "      <th>Taid</th>\n",
       "      <th>Poppous</th>\n",
       "      <th>Koroua</th>\n",
       "      <th>Abuela</th>\n",
       "      <th>Amona</th>\n",
       "      <th>Sabo</th>\n",
       "      <th>Mimer</th>\n",
       "      <th>Crus</th>\n",
       "      <th>...</th>\n",
       "      <th>Vims</th>\n",
       "      <th>Echoes</th>\n",
       "      <th>Galvinrad</th>\n",
       "      <th>Uther</th>\n",
       "      <th>Rip</th>\n",
       "      <th>Rup</th>\n",
       "      <th>Slaad</th>\n",
       "      <th>Pantea</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Zann</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J14_21_reoriented</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J53_21_reoriented</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J105_22_reoriented</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J46_21_reoriented</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J50_21_reoriented</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                phage Ymer Taid Poppous Koroua Abuela Amona Sabo Mimer  Crus  \\\n",
       "0   J14_21_reoriented  NaN  NaN     NaN    NaN    NaN   NaN  NaN   NaN   NaN   \n",
       "1   J53_21_reoriented  NaN  NaN     NaN    NaN    NaN   NaN  NaN   NaN   NaN   \n",
       "2  J105_22_reoriented  NaN  NaN     NaN    NaN    NaN   NaN  NaN   NaN   NaN   \n",
       "3   J46_21_reoriented  NaN  NaN     NaN    NaN    NaN   NaN  NaN   NaN   NaN   \n",
       "4   J50_21_reoriented  NaN  NaN     NaN    NaN    NaN   NaN  NaN   NaN   NaN   \n",
       "\n",
       "   ... Vims Echoes Galvinrad Uther  Rip  Rup Slaad Pantea  Rap Zann  \n",
       "0  ...  NaN    NaN       NaN   NaN  NaN  NaN   NaN    NaN  NaN  NaN  \n",
       "1  ...  NaN    NaN       NaN   NaN  NaN  NaN   NaN    NaN  NaN  NaN  \n",
       "2  ...  NaN    NaN       NaN   NaN  NaN  NaN   NaN    NaN  NaN  NaN  \n",
       "3  ...  NaN    NaN       NaN   NaN  NaN  NaN   NaN    NaN  NaN  NaN  \n",
       "4  ...  NaN    NaN       NaN   NaN  NaN  NaN   NaN    NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Ymer': nan,\n",
       " 'Taid': nan,\n",
       " 'Poppous': nan,\n",
       " 'Koroua': nan,\n",
       " 'Abuela': nan,\n",
       " 'Amona': nan,\n",
       " 'Sabo': nan,\n",
       " 'Mimer': nan,\n",
       " 'Crus': nan,\n",
       " 'Gander': nan,\n",
       " 'Guf': nan,\n",
       " 'Hoejben': nan,\n",
       " 'Magnum': 200000000,\n",
       " 'Vims': nan,\n",
       " 'Echoes': nan,\n",
       " 'Galvinrad': nan,\n",
       " 'Uther': nan,\n",
       " 'Rip': nan,\n",
       " 'Rup': nan,\n",
       " 'Slaad': nan,\n",
       " 'Pantea': nan,\n",
       " 'Rap': nan,\n",
       " 'Zann': nan}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the host range data from the Excel file\n",
    "file_path = raw_data_path + \"phagehost_KU/Hostrange_data_all_crisp_iso.xlsx\"\n",
    "sheet_name = \"sum_hostrange\"  # replace with your sheet name\n",
    "host_range_df = pd.read_excel(\n",
    "    file_path,\n",
    "    sheet_name='sum_hostrange',\n",
    "    header=1).drop(columns=[\"isolate ID\", \"Hostrange_analysis\", \"Phage\"])\n",
    "\n",
    "# Create a lookup dictionary for bacteria species based on Seq ID - dict\n",
    "bact_lookup = host_range_df[[\"Seq ID\", \"Species\"]].drop_duplicates(subset=['Seq ID']).set_index('Seq ID').to_dict()['Species']\n",
    "print(\"Bacteria lookup dictionary created with\", len(bact_lookup), \"entries.\")\n",
    "print(bact_lookup)\n",
    "\n",
    "# Make Seq ID to phage name mapping - pandas df\n",
    "host_range_df = host_range_df.drop(columns=[\"Species\"]).set_index('Seq ID').rename_axis('phage').reset_index()\n",
    "display(host_range_df.head())\n",
    "\n",
    "# Convert the host range data into a dictionary\n",
    "host_range_data = {}\n",
    "for index, row in host_range_df.iterrows():\n",
    "    cleaned_index = row[1:].index.str.replace(\" \", \"\")\n",
    "    curr_bact_series = row[1:]\n",
    "    curr_bact_series.index = cleaned_index\n",
    "    host_range_data[row['phage']] = curr_bact_series.to_dict()\n",
    "\n",
    "host_range_data[\"J10_21_reoriented\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c776c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_host_range(host_range_dict):\n",
    "    binary_dict = {}\n",
    "    for host, val in host_range_dict.items():\n",
    "        if pd.isna(val) or val == 0:\n",
    "            binary_dict[host] = 0\n",
    "        else:\n",
    "            binary_dict[host] = 1\n",
    "    return binary_dict\n",
    "\n",
    "def short_species_name(full_name):\n",
    "    if len(full_name.split(\" \")) < 2:\n",
    "        return full_name\n",
    "    else:\n",
    "        return full_name.split(\" \")[0][0] + \".\" + full_name.split(\" \")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ba003",
   "metadata": {},
   "source": [
    "Loading MinHash sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698ee6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing host range data for bacteria species: Pectobacterium punjabense\n",
      "Seq IDs for selected species: ['J2_21_reoriented', 'J3_21_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Uther_P.punjabense...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Echoes_P.punjabense...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Gander_P.punjabense...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Ymer_P.punjabense...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Vims_P.punjabense...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Pantea_P.punjabense...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Sabo_P.punjabense...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Magnum_P.punjabense...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Mimer_P.punjabense...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Abuela_P.punjabense...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Poppous_P.punjabense...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Rap_P.punjabense...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Taid_P.punjabense...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Rup_P.punjabense...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Galvinrad_P.punjabense...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Crus_P.punjabense...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Guf_P.punjabense...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Amona_P.punjabense...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Koroua_P.punjabense...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Slaad_P.punjabense...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Hoejben_P.punjabense...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Zann_P.punjabense...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_punjabense/+Rip_P.punjabense...\n",
      "\n",
      "Processing host range data for bacteria species: Lelliottia\n",
      "Seq IDs for selected species: ['J1_21_reoriented', 'J91_22_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 1, 'Zann': 1}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Uther_Lelliottia...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Echoes_Lelliottia...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Gander_Lelliottia...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Ymer_Lelliottia...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Vims_Lelliottia...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Pantea_Lelliottia...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Sabo_Lelliottia...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Magnum_Lelliottia...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Mimer_Lelliottia...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Abuela_Lelliottia...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Poppous_Lelliottia...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Rap_Lelliottia...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Taid_Lelliottia...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Rup_Lelliottia...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Galvinrad_Lelliottia...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Crus_Lelliottia...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Guf_Lelliottia...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Amona_Lelliottia...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Koroua_Lelliottia...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Slaad_Lelliottia...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Hoejben_Lelliottia...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Zann_Lelliottia...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Lelliottia/+Rip_Lelliottia...\n",
      "\n",
      "Processing host range data for bacteria species: Vagococcus\n",
      "Seq IDs for selected species: ['J5_21_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Uther_Vagococcus...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Echoes_Vagococcus...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Gander_Vagococcus...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Ymer_Vagococcus...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Vims_Vagococcus...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Pantea_Vagococcus...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Sabo_Vagococcus...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Magnum_Vagococcus...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Mimer_Vagococcus...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Abuela_Vagococcus...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Poppous_Vagococcus...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Rap_Vagococcus...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Taid_Vagococcus...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Rup_Vagococcus...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Galvinrad_Vagococcus...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Crus_Vagococcus...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Guf_Vagococcus...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Amona_Vagococcus...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Koroua_Vagococcus...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Slaad_Vagococcus...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Hoejben_Vagococcus...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Zann_Vagococcus...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Vagococcus/+Rip_Vagococcus...\n",
      "\n",
      "Processing host range data for bacteria species: Chishuiella\n",
      "Seq IDs for selected species: ['J105_22_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Uther_Chishuiella...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Echoes_Chishuiella...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Gander_Chishuiella...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Ymer_Chishuiella...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Vims_Chishuiella...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Pantea_Chishuiella...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Sabo_Chishuiella...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Magnum_Chishuiella...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Mimer_Chishuiella...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Abuela_Chishuiella...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Poppous_Chishuiella...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Rap_Chishuiella...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Taid_Chishuiella...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Rup_Chishuiella...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Galvinrad_Chishuiella...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Crus_Chishuiella...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Guf_Chishuiella...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Amona_Chishuiella...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Koroua_Chishuiella...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Slaad_Chishuiella...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Hoejben_Chishuiella...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Zann_Chishuiella...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chishuiella/+Rip_Chishuiella...\n",
      "\n",
      "Processing host range data for bacteria species: Pseudomonas marginalis\n",
      "Seq IDs for selected species: ['J101_22_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Uther_P.marginalis...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Echoes_P.marginalis...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Gander_P.marginalis...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Ymer_P.marginalis...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Vims_P.marginalis...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Pantea_P.marginalis...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Sabo_P.marginalis...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Magnum_P.marginalis...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Mimer_P.marginalis...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Abuela_P.marginalis...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Poppous_P.marginalis...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Rap_P.marginalis...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Taid_P.marginalis...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Rup_P.marginalis...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Galvinrad_P.marginalis...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Crus_P.marginalis...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Guf_P.marginalis...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Amona_P.marginalis...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Koroua_P.marginalis...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Slaad_P.marginalis...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Hoejben_P.marginalis...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Zann_P.marginalis...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_marginalis/+Rip_P.marginalis...\n",
      "\n",
      "Processing host range data for bacteria species: Pectobacterium atrosepticum\n",
      "Seq IDs for selected species: ['J10_21_reoriented', 'J11_21_reoriented', 'J126_23_reoriented', 'J12_21_reoriented', 'J16_21_reoriented', 'J22_21_reoriented', 'J28_21_reoriented', 'J33_21_reoriented', 'J38_21_reoriented', 'J39_21_reoriented', 'J40_21_reoriented', 'J56_21_reoriented', 'J6_21_reoriented', 'J7_21_reoriented', 'J8_21_reoriented', 'J116_23_reoriented', 'J118_23_reoriented', 'J121_23_reoriented', 'J125_23_reoriented', 'J70_21_reoriented', 'J78_22_reoriented', 'J81_22_reoriented', 'J82_22_reoriented', 'J84_22_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 1, 'Taid': 1, 'Poppous': 1, 'Koroua': 1, 'Abuela': 1, 'Amona': 1, 'Sabo': 0, 'Mimer': 1, 'Crus': 1, 'Gander': 1, 'Guf': 1, 'Hoejben': 1, 'Magnum': 1, 'Vims': 1, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 1, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Uther_P.atrosepticum...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Echoes_P.atrosepticum...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Gander_P.atrosepticum...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Ymer_P.atrosepticum...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Vims_P.atrosepticum...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Pantea_P.atrosepticum...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Sabo_P.atrosepticum...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Magnum_P.atrosepticum...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Mimer_P.atrosepticum...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Abuela_P.atrosepticum...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Poppous_P.atrosepticum...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Rap_P.atrosepticum...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Taid_P.atrosepticum...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Rup_P.atrosepticum...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Galvinrad_P.atrosepticum...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Crus_P.atrosepticum...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Guf_P.atrosepticum...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Amona_P.atrosepticum...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Koroua_P.atrosepticum...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Slaad_P.atrosepticum...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Hoejben_P.atrosepticum...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Zann_P.atrosepticum...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_atrosepticum/+Rip_P.atrosepticum...\n",
      "\n",
      "Processing host range data for bacteria species: Acinetobacter calcoaceticus\n",
      "Seq IDs for selected species: ['J14_21_reoriented', 'J53_21_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Uther_A.calcoaceticus...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Echoes_A.calcoaceticus...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Gander_A.calcoaceticus...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Ymer_A.calcoaceticus...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Vims_A.calcoaceticus...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Pantea_A.calcoaceticus...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Sabo_A.calcoaceticus...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Magnum_A.calcoaceticus...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Mimer_A.calcoaceticus...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Abuela_A.calcoaceticus...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Poppous_A.calcoaceticus...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Rap_A.calcoaceticus...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Taid_A.calcoaceticus...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Rup_A.calcoaceticus...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Galvinrad_A.calcoaceticus...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Crus_A.calcoaceticus...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Guf_A.calcoaceticus...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Amona_A.calcoaceticus...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Koroua_A.calcoaceticus...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Slaad_A.calcoaceticus...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Hoejben_A.calcoaceticus...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Zann_A.calcoaceticus...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Acinetobacter_calcoaceticus/+Rip_A.calcoaceticus...\n",
      "\n",
      "Processing host range data for bacteria species: Pectobacterium polaris\n",
      "Seq IDs for selected species: ['J17_21_reoriented', 'J18_21_reoriented', 'J19_21_reoriented', 'J48_21_reoriented', 'J3009_1_22_KMC_reoriented', 'J60_21_reoriented', 'J61_21_reoriented', 'J66_21_reoriented', 'J75_21_reoriented', 'J76_21_reoriented', 'J79_22_reoriented', 'J94_22_reoriented', 'J99_22_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 1, 'Taid': 1, 'Poppous': 1, 'Koroua': 1, 'Abuela': 1, 'Amona': 1, 'Sabo': 0, 'Mimer': 1, 'Crus': 1, 'Gander': 1, 'Guf': 1, 'Hoejben': 1, 'Magnum': 1, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 1, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 1, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Uther_P.polaris...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Echoes_P.polaris...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Gander_P.polaris...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Ymer_P.polaris...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Vims_P.polaris...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Pantea_P.polaris...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Sabo_P.polaris...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Magnum_P.polaris...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Mimer_P.polaris...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Abuela_P.polaris...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Poppous_P.polaris...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Rap_P.polaris...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Taid_P.polaris...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Rup_P.polaris...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Galvinrad_P.polaris...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Crus_P.polaris...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Guf_P.polaris...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Amona_P.polaris...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Koroua_P.polaris...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Slaad_P.polaris...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Hoejben_P.polaris...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Zann_P.polaris...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_polaris/+Rip_P.polaris...\n",
      "\n",
      "Processing host range data for bacteria species: Pseudomonas chlororaphis\n",
      "Seq IDs for selected species: ['J109_23_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Uther_P.chlororaphis...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Echoes_P.chlororaphis...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Gander_P.chlororaphis...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Ymer_P.chlororaphis...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Vims_P.chlororaphis...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Pantea_P.chlororaphis...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Sabo_P.chlororaphis...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Magnum_P.chlororaphis...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Mimer_P.chlororaphis...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Abuela_P.chlororaphis...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Poppous_P.chlororaphis...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Rap_P.chlororaphis...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Taid_P.chlororaphis...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Rup_P.chlororaphis...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Galvinrad_P.chlororaphis...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Crus_P.chlororaphis...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Guf_P.chlororaphis...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Amona_P.chlororaphis...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Koroua_P.chlororaphis...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Slaad_P.chlororaphis...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Hoejben_P.chlororaphis...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Zann_P.chlororaphis...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pseudomonas_chlororaphis/+Rip_P.chlororaphis...\n",
      "\n",
      "Processing host range data for bacteria species: Serratia liquefaciens\n",
      "Seq IDs for selected species: ['J15_21_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 1, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Uther_S.liquefaciens...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Echoes_S.liquefaciens...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Gander_S.liquefaciens...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Ymer_S.liquefaciens...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Vims_S.liquefaciens...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Pantea_S.liquefaciens...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Sabo_S.liquefaciens...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Magnum_S.liquefaciens...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Mimer_S.liquefaciens...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Abuela_S.liquefaciens...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Poppous_S.liquefaciens...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Rap_S.liquefaciens...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Taid_S.liquefaciens...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Rup_S.liquefaciens...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Galvinrad_S.liquefaciens...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Crus_S.liquefaciens...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Guf_S.liquefaciens...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Amona_S.liquefaciens...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Koroua_S.liquefaciens...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Slaad_S.liquefaciens...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Hoejben_S.liquefaciens...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Zann_S.liquefaciens...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_liquefaciens/+Rip_S.liquefaciens...\n",
      "\n",
      "Processing host range data for bacteria species: Pectobacterium brasiliense\n",
      "Seq IDs for selected species: ['J119_23_reoriented', 'J127_23_reoriented', 'J13_21_reoriented', 'J20_21_reoriented', 'J21_21_reoriented', 'J24_21_reoriented', 'J25_21_reoriented', 'J26_21_reoriented', 'J31_21_reoriented', 'J32_21_reoriented', 'J34_21_reoriented', 'J36_21_reoriented', 'J37_21_reoriented', 'J41_21_reoriented', 'J44_21_reoriented', 'J45_21_reoriented', 'J47_21_reoriented', 'J52_21_reoriented', 'J59_21_reoriented', 'J115_23_reoriented', 'J117_23_reoriented', 'J128_23_reoriented', 'J134_23_reoriented', 'J3009LEM_22_KMC_reoriented', 'J3009_2_22_KMC_reoriented', 'J69_21_reoriented', 'J73_21_reoriented', 'J77_22_reoriented', 'J86_22_reoriented', 'J97_22_reoriented', 'J98_22_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 1, 'Taid': 1, 'Poppous': 1, 'Koroua': 1, 'Abuela': 1, 'Amona': 1, 'Sabo': 1, 'Mimer': 1, 'Crus': 1, 'Gander': 1, 'Guf': 1, 'Hoejben': 1, 'Magnum': 1, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 1, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Uther_P.brasiliense...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Echoes_P.brasiliense...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Gander_P.brasiliense...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Ymer_P.brasiliense...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Vims_P.brasiliense...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Pantea_P.brasiliense...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Sabo_P.brasiliense...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Magnum_P.brasiliense...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Mimer_P.brasiliense...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Abuela_P.brasiliense...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Poppous_P.brasiliense...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Rap_P.brasiliense...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Taid_P.brasiliense...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Rup_P.brasiliense...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Galvinrad_P.brasiliense...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Crus_P.brasiliense...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Guf_P.brasiliense...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Amona_P.brasiliense...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Koroua_P.brasiliense...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Slaad_P.brasiliense...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Hoejben_P.brasiliense...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Zann_P.brasiliense...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_brasiliense/+Rip_P.brasiliense...\n",
      "\n",
      "Processing host range data for bacteria species: Chryseobacterium\n",
      "Seq IDs for selected species: ['J46_21_reoriented', 'J50_21_reoriented', 'J2264_1_22_KMC_reoriented', 'J2264_3_22_KMC_reoriented', 'J63_22_reoriented', 'J64_22_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Uther_Chryseobacterium...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Echoes_Chryseobacterium...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Gander_Chryseobacterium...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Ymer_Chryseobacterium...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Vims_Chryseobacterium...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Pantea_Chryseobacterium...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Sabo_Chryseobacterium...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Magnum_Chryseobacterium...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Mimer_Chryseobacterium...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Abuela_Chryseobacterium...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Poppous_Chryseobacterium...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Rap_Chryseobacterium...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Taid_Chryseobacterium...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Rup_Chryseobacterium...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Galvinrad_Chryseobacterium...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Crus_Chryseobacterium...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Guf_Chryseobacterium...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Amona_Chryseobacterium...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Koroua_Chryseobacterium...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Slaad_Chryseobacterium...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Hoejben_Chryseobacterium...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Zann_Chryseobacterium...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Chryseobacterium/+Rip_Chryseobacterium...\n",
      "\n",
      "Processing host range data for bacteria species: Pectobacterium parmentieri\n",
      "Seq IDs for selected species: ['J27_21_reoriented', 'J29_21_reoriented', 'J30_21_reoriented', 'J42_21_reoriented', 'J49_21_reoriented', 'FO3A_23_KMC_reoriented', 'J103_22_reoriented', 'J104_22_reoriented', 'J106_22_reoriented', 'J108_22_reoriented', 'J62_22_reoriented', 'J65_21_reoriented', 'J68_21_reoriented', 'J83_22_reoriented', 'J93_22_reoriented', 'J95_22_reoriented', 'J96_22_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 1, 'Taid': 1, 'Poppous': 1, 'Koroua': 1, 'Abuela': 1, 'Amona': 1, 'Sabo': 0, 'Mimer': 1, 'Crus': 1, 'Gander': 1, 'Guf': 1, 'Hoejben': 1, 'Magnum': 1, 'Vims': 1, 'Echoes': 0, 'Galvinrad': 1, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 1, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Uther_P.parmentieri...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Echoes_P.parmentieri...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Gander_P.parmentieri...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Ymer_P.parmentieri...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Vims_P.parmentieri...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Pantea_P.parmentieri...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Sabo_P.parmentieri...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Magnum_P.parmentieri...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Mimer_P.parmentieri...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Abuela_P.parmentieri...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Poppous_P.parmentieri...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Rap_P.parmentieri...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Taid_P.parmentieri...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Rup_P.parmentieri...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Galvinrad_P.parmentieri...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Crus_P.parmentieri...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Guf_P.parmentieri...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Amona_P.parmentieri...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Koroua_P.parmentieri...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Slaad_P.parmentieri...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Hoejben_P.parmentieri...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Zann_P.parmentieri...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_parmentieri/+Rip_P.parmentieri...\n",
      "\n",
      "Processing host range data for bacteria species: Serratia plymuthica\n",
      "Seq IDs for selected species: ['J4_21_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 1, 'Uther': 1, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Uther_S.plymuthica...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Echoes_S.plymuthica...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Gander_S.plymuthica...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Ymer_S.plymuthica...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Vims_S.plymuthica...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Pantea_S.plymuthica...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Sabo_S.plymuthica...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Magnum_S.plymuthica...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Mimer_S.plymuthica...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Abuela_S.plymuthica...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Poppous_S.plymuthica...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Rap_S.plymuthica...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Taid_S.plymuthica...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Rup_S.plymuthica...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Galvinrad_S.plymuthica...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Crus_S.plymuthica...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Guf_S.plymuthica...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Amona_S.plymuthica...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Koroua_S.plymuthica...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Slaad_S.plymuthica...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Hoejben_S.plymuthica...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Zann_S.plymuthica...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Serratia_plymuthica/+Rip_S.plymuthica...\n",
      "\n",
      "Processing host range data for bacteria species: Pectobacterium carotovorum\n",
      "Seq IDs for selected species: ['J54_21_reoriented', 'J100_22_reoriented', 'J102_22_reoriented', 'J74_21_reoriented', 'J80_22_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 1, 'Taid': 0, 'Poppous': 0, 'Koroua': 1, 'Abuela': 1, 'Amona': 1, 'Sabo': 0, 'Mimer': 1, 'Crus': 1, 'Gander': 1, 'Guf': 1, 'Hoejben': 1, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 0, 'Rup': 0, 'Slaad': 0, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Uther_P.carotovorum...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Echoes_P.carotovorum...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Gander_P.carotovorum...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Ymer_P.carotovorum...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Vims_P.carotovorum...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Pantea_P.carotovorum...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Sabo_P.carotovorum...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Magnum_P.carotovorum...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Mimer_P.carotovorum...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Abuela_P.carotovorum...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Poppous_P.carotovorum...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Rap_P.carotovorum...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Taid_P.carotovorum...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Rup_P.carotovorum...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Galvinrad_P.carotovorum...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Crus_P.carotovorum...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Guf_P.carotovorum...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Amona_P.carotovorum...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Koroua_P.carotovorum...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Slaad_P.carotovorum...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Hoejben_P.carotovorum...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Zann_P.carotovorum...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Pectobacterium_carotovorum/+Rip_P.carotovorum...\n",
      "\n",
      "Processing host range data for bacteria species: Morganella morganii\n",
      "Seq IDs for selected species: ['J51_21_reoriented', 'J57_21_reoriented']\n",
      "Combined host range data for selected species: {'Ymer': 0, 'Taid': 0, 'Poppous': 0, 'Koroua': 0, 'Abuela': 0, 'Amona': 0, 'Sabo': 0, 'Mimer': 0, 'Crus': 0, 'Gander': 0, 'Guf': 0, 'Hoejben': 0, 'Magnum': 0, 'Vims': 0, 'Echoes': 0, 'Galvinrad': 0, 'Uther': 0, 'Rip': 1, 'Rup': 1, 'Slaad': 1, 'Pantea': 0, 'Rap': 0, 'Zann': 0}\n",
      "\n",
      "Loading sketches from: ../data_prod/phage_minhash_12/\n",
      "Loaded sketches for 23 phages.\n",
      "Phage: Uther, Number of hashes: 43422\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Uther_M.morganii...\n",
      "Phage: Echoes, Number of hashes: 58571\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Echoes_M.morganii...\n",
      "Phage: Gander, Number of hashes: 43262\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Gander_M.morganii...\n",
      "Phage: Ymer, Number of hashes: 40888\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Ymer_M.morganii...\n",
      "Phage: Vims, Number of hashes: 45255\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Vims_M.morganii...\n",
      "Phage: Pantea, Number of hashes: 145070\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Pantea_M.morganii...\n",
      "Phage: Sabo, Number of hashes: 41784\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Sabo_M.morganii...\n",
      "Phage: Magnum, Number of hashes: 90672\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Magnum_M.morganii...\n",
      "Phage: Mimer, Number of hashes: 5768\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Mimer_M.morganii...\n",
      "Phage: Abuela, Number of hashes: 41116\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Abuela_M.morganii...\n",
      "Phage: Poppous, Number of hashes: 40547\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Poppous_M.morganii...\n",
      "Phage: Rap, Number of hashes: 39939\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Rap_M.morganii...\n",
      "Phage: Taid, Number of hashes: 39487\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Taid_M.morganii...\n",
      "Phage: Rup, Number of hashes: 60278\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Rup_M.morganii...\n",
      "Phage: Galvinrad, Number of hashes: 30932\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Galvinrad_M.morganii...\n",
      "Phage: Crus, Number of hashes: 43737\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Crus_M.morganii...\n",
      "Phage: Guf, Number of hashes: 43531\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Guf_M.morganii...\n",
      "Phage: Amona, Number of hashes: 41835\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Amona_M.morganii...\n",
      "Phage: Koroua, Number of hashes: 40896\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Koroua_M.morganii...\n",
      "Phage: Slaad, Number of hashes: 43199\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Slaad_M.morganii...\n",
      "Phage: Hoejben, Number of hashes: 43827\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Hoejben_M.morganii...\n",
      "Phage: Zann, Number of hashes: 50015\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Zann_M.morganii...\n",
      "Phage: Rip, Number of hashes: 58607\n",
      "Writing ../data_prod/phage_minhash_12_txt/Morganella_morganii/+Rip_M.morganii...\n"
     ]
    }
   ],
   "source": [
    "from sourmash import load_one_signature\n",
    "import shutil\n",
    "\n",
    "parent_out_dir = data_prod_path + f\"phage_minhash_{K}_txt/\"\n",
    "if not os.path.exists(parent_out_dir):\n",
    "    os.makedirs(parent_out_dir)\n",
    "else:\n",
    "    shutil.rmtree(parent_out_dir)\n",
    "    os.makedirs(parent_out_dir)\n",
    "\n",
    "for selected_bact_species in set(bact_lookup.values()): \n",
    "    ### PREPPING HOST RANGE DATA ###\n",
    "    # Select a specific bacteria species for host range analysis\n",
    "    print(f\"\\nProcessing host range data for bacteria species: {selected_bact_species}\")\n",
    "\n",
    "    #obtain all the keys where the value is equal to selected_bact_species\n",
    "    selected_seqIDs = [key for key, value in bact_lookup.items() if value == selected_bact_species]\n",
    "    print(\"Seq IDs for selected species:\", selected_seqIDs)\n",
    "\n",
    "    # Acceptive approach: since all seqIDs for the same species should have similar host ranges, we combine their host range data.\n",
    "    # if non-zero is found for any seqID, we set it to 1 in the final host range data.\n",
    "    def combine_host_ranges(seqID_list, approach=\"acceptive\", threshold=0.5, TS = False):\n",
    "        combined_host_range = {}\n",
    "        # Acceptive approach: if any seqID has a non-zero value for a host, set to 1\n",
    "        if approach == \"acceptive\":\n",
    "            for seqID in seqID_list:\n",
    "                curr_host_range = binarize_host_range(host_range_data[seqID])\n",
    "                for host, val in curr_host_range.items():\n",
    "                    if host not in combined_host_range:\n",
    "                        combined_host_range[host] = val\n",
    "                    else:\n",
    "                        if not pd.isna(val) and val != 0:\n",
    "                            combined_host_range[host] = 1\n",
    "            return combined_host_range\n",
    "        \n",
    "        # Count occurrences of non-zero values for each host, if higher than threshold, set to 1\n",
    "        elif approach == \"consensus\":\n",
    "            host_counts = {}\n",
    "            for seqID in seqID_list:\n",
    "                curr_host_range = binarize_host_range(host_range_data[seqID])\n",
    "                for host, val in curr_host_range.items():\n",
    "                    if host not in host_counts:\n",
    "                        host_counts[host] = 0\n",
    "                    if not pd.isna(val) and val != 0:\n",
    "                        host_counts[host] += 1\n",
    "            for host, count in host_counts.items():\n",
    "                if TS: print(f\"Host: {host}, Count: {count}, Total SeqIDs: {len(seqID_list)}, Ratio: {count / len(seqID_list)}\")\n",
    "                if count / len(seqID_list) >= threshold:\n",
    "                    combined_host_range[host] = 1\n",
    "                else:\n",
    "                    combined_host_range[host] = 0\n",
    "            return combined_host_range\n",
    "\n",
    "    combined_host_range = combine_host_ranges(selected_seqIDs, approach=\"acceptive\")\n",
    "    print(\"Combined host range data for selected species:\", combined_host_range)\n",
    "\n",
    "    ### LOADING MINHASH SKETCHES ###\n",
    "    minhash_data = {}\n",
    "    print(f\"\\nLoading sketches from: {SKETCH_DIR}\")\n",
    "    for filename in os.listdir(SKETCH_DIR):\n",
    "        if filename.endswith(('.sig', '.json')): # sourmash signature files\n",
    "            filepath = os.path.join(SKETCH_DIR, filename)\n",
    "            try:\n",
    "                # sourmash.load_signatures returns an iterator\n",
    "                sig = load_one_signature(filepath, K)\n",
    "                \n",
    "                if not sig:\n",
    "                    print(f\"Warning: No signatures found in {filename}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                phage_name = str(sig)\n",
    "                \n",
    "                if phage_name in combined_host_range:\n",
    "                    # Extract the hash values (sorted for consistency)\n",
    "                    hashes = sorted(sig.minhash.hashes.keys())\n",
    "                    minhash_data[phage_name] = hashes\n",
    "                else:\n",
    "                    print(f\"Warning: Sketch for {phage_name} found, but no matching entry in host range data. Skipping.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading sketch file {filename}: {e}. Skipping.\")\n",
    "\n",
    "    print(f\"Loaded sketches for {len(minhash_data)} phages.\")\n",
    "\n",
    "    ### OUTPUTTING MINHASH TXT FILES ###\n",
    "    out_dir = parent_out_dir + f\"{str(selected_bact_species).replace(' ', '_')}/\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    for phage_name in minhash_data.keys():\n",
    "        print(f\"Phage: {phage_name}, Number of hashes: {len(minhash_data[phage_name])}\")\n",
    "        print(f\"Writing {out_dir}+{phage_name}_{short_species_name(selected_bact_species)}...\")\n",
    "        with open(os.path.join(out_dir, f\"{phage_name}.txt\"), 'w') as f:\n",
    "            for hash_value in minhash_data[phage_name]:\n",
    "                f.write(f\"{hash_value}\\t{combined_host_range[phage_name]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6193f1",
   "metadata": {},
   "source": [
    "### Make combined txt files per bacteria\n",
    "each file will contain which minhashes has been in a phage that could infect the bacteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40a80910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sketches for Chryseobacterium into ../data_prod/phage_minhash_12_combined/Chryseobacterium_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Lelliottia into ../data_prod/phage_minhash_12_combined/Lelliottia_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Acinetobacter_calcoaceticus into ../data_prod/phage_minhash_12_combined/Acinetobacter_calcoaceticus_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Chishuiella into ../data_prod/phage_minhash_12_combined/Chishuiella_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Vagococcus into ../data_prod/phage_minhash_12_combined/Vagococcus_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Pectobacterium_polaris into ../data_prod/phage_minhash_12_combined/Pectobacterium_polaris_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Pseudomonas_chlororaphis into ../data_prod/phage_minhash_12_combined/Pseudomonas_chlororaphis_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Morganella_morganii into ../data_prod/phage_minhash_12_combined/Morganella_morganii_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Pectobacterium_parmentieri into ../data_prod/phage_minhash_12_combined/Pectobacterium_parmentieri_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Pectobacterium_brasiliense into ../data_prod/phage_minhash_12_combined/Pectobacterium_brasiliense_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Pectobacterium_atrosepticum into ../data_prod/phage_minhash_12_combined/Pectobacterium_atrosepticum_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Pseudomonas_marginalis into ../data_prod/phage_minhash_12_combined/Pseudomonas_marginalis_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Pectobacterium_carotovorum into ../data_prod/phage_minhash_12_combined/Pectobacterium_carotovorum_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Serratia_plymuthica into ../data_prod/phage_minhash_12_combined/Serratia_plymuthica_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Pectobacterium_punjabense into ../data_prod/phage_minhash_12_combined/Pectobacterium_punjabense_combined.txt\n",
      "Shuffling combined file for random distribution...\n",
      "Combined sketches for Serratia_liquefaciens into ../data_prod/phage_minhash_12_combined/Serratia_liquefaciens_combined.txt\n",
      "Shuffling combined file for random distribution...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "### Concatenate all phage minhash txt files into a single file for easier NN training ###\n",
    "all_phage_minhash_txt_path = data_prod_path + f\"phage_minhash_{K}_combined/\"\n",
    "if not os.path.exists(all_phage_minhash_txt_path):\n",
    "    os.makedirs(all_phage_minhash_txt_path)\n",
    "\n",
    "for filename in os.listdir(parent_out_dir):\n",
    "    if os.path.isdir(os.path.join(parent_out_dir, filename)):\n",
    "        bact_folder = os.path.join(parent_out_dir, filename)\n",
    "        combined_output_file = os.path.join(all_phage_minhash_txt_path, f\"{filename}_combined.txt\")\n",
    "        with open(combined_output_file, 'w') as outfile:\n",
    "            for phage_file in os.listdir(bact_folder):\n",
    "                if phage_file.endswith('.txt'):\n",
    "                    phage_filepath = os.path.join(bact_folder, phage_file)\n",
    "                    with open(phage_filepath, 'r') as infile:\n",
    "                        shutil.copyfileobj(infile, outfile)\n",
    "        print(f\"Combined sketches for {filename} into {combined_output_file}\")\n",
    "        \n",
    "        print(\"Shuffling combined file for random distribution...\")\n",
    "        with open(combined_output_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        random.shuffle(lines)\n",
    "        with open(combined_output_file, 'w') as f:\n",
    "            f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6ce13",
   "metadata": {},
   "source": [
    "## Running NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943fed2",
   "metadata": {},
   "source": [
    "Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8ea637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages -----------------------------\n",
    "import random, shutil, os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Paths --------------------------------\n",
    "raw_data_path = \"../raw_data/\"\n",
    "data_prod_path = \"../data_prod/\"\n",
    "NN_files_path = data_prod_path + \"NN_files/\"\n",
    "if not os.path.exists(NN_files_path):\n",
    "    os.makedirs(NN_files_path)\n",
    "else:\n",
    "    shutil.rmtree(NN_files_path)\n",
    "    os.makedirs(NN_files_path)\n",
    "\n",
    "### Custom variables ---------------------\n",
    "K = 12 #kmer size; equal to 6 aa.\n",
    "selected_bact_species = \"Pectobacterium brasiliense\" \n",
    "full_path = data_prod_path+f\"phage_minhash_{K}_combined/{selected_bact_species.replace(' ', '_')}_combined.txt\"\n",
    "if not os.path.exists(full_path):\n",
    "    raise FileNotFoundError(\"Combined minhash txt file for the selected bacteria species not found. Please run the data preparation steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338941d",
   "metadata": {},
   "source": [
    "### Splitting data\n",
    "each folder in phage_minhash_K_txt is a bacteria name, specifying whether the sketches of each underlying phage txt file, can infect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27b3fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_test(path, train_val_ratio=0.9, test_ratio=0.1, TS = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Separate the input file into training/validation and test sets based on specified ratios.\n",
    "    Write the separated data to respective train_val and test files.\n",
    "    Returns the paths to the train_val and test files.\n",
    "    \"\"\"\n",
    "    if abs(train_val_ratio + test_ratio - 1.0) > 1e-6:\n",
    "        raise ValueError(\"Train, validation, and test ratios must sum to 1.\")\n",
    "\n",
    "    if type(path) == list:\n",
    "        lines = []\n",
    "        for input_file in path:\n",
    "            if TS: print(f\"Separating data from {input_file} into train/val and test sets...\")\n",
    "            try: \n",
    "                with open(input_file, 'r') as f:\n",
    "                    lines.extend(f.readlines())\n",
    "            except FileNotFoundError as e:\n",
    "                print(\"File not found: \", e)\n",
    "    else:\n",
    "        if TS: print(f\"Separating data from {path} into train/val and test sets...\")\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    total_lines = len(lines)\n",
    "    train_val_end = int(total_lines * train_val_ratio)\n",
    "\n",
    "    train_data = lines[:train_val_end]\n",
    "    test_data = lines[train_val_end:]\n",
    "    \n",
    "    if TS: \n",
    "        print(f\"Total lines: {total_lines}, Train/Val lines: {len(train_data)}, Test lines: {len(test_data)}\")\n",
    "        print(f\"Ratios - Train/Val: {len(train_data)/total_lines:.2f}, Test: {len(test_data)/total_lines:.2f}\", end=\"\\n\\n\")\n",
    "\n",
    "    # Write the separated data to respective files to save memory\n",
    "    train_val_path = NN_files_path + f\"{selected_bact_species.replace(' ', '_')}_train_val.txt\"\n",
    "    with open(NN_files_path + f\"{selected_bact_species.replace(' ', '_')}_train_val.txt\", 'w') as f:\n",
    "        f.writelines(train_data)\n",
    "    \n",
    "    test_path = NN_files_path + f\"{selected_bact_species.replace(' ', '_')}_test.txt\"\n",
    "    with open(NN_files_path + f\"{selected_bact_species.replace(' ', '_')}_test.txt\", 'w') as f:\n",
    "        f.writelines(test_data)\n",
    "    \n",
    "    return train_val_path, test_path\n",
    "\n",
    "def lines_to_df(lines):\n",
    "    rows = [l.strip().split('\\t') for l in lines if l.strip()]\n",
    "    df = pd.DataFrame(rows, columns=['kmer_hash', 'label'])\n",
    "    # set proper dtypes\n",
    "    df['kmer_hash'] = pd.to_numeric(df['kmer_hash'], errors='coerce').astype(np.uint64)\n",
    "    df['label'] = pd.to_numeric(df['label'], errors='coerce').astype(int)\n",
    "    return df\n",
    "\n",
    "def k_fold_split(train_val_file, k=5, seed=42):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation split on the input file.\n",
    "    Returns a list of (train_data, val_data) tuples for each fold.\n",
    "    Does not return test data.\n",
    "    \"\"\"    \n",
    "    # Read all the shuffled train val data\n",
    "    with open(train_val_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Compute fold sizes\n",
    "    total = len(lines)\n",
    "    fold_size = total // k\n",
    "    folds = [lines[i*fold_size:(i+1)*fold_size] for i in range(k)]\n",
    "\n",
    "    # Handle remainder lines (if not evenly divisible)\n",
    "    remainder = lines[k*fold_size:]\n",
    "    for i, line in enumerate(remainder):\n",
    "        folds[i % k].append(line)\n",
    "\n",
    "    # Generate (train, val) pairs\n",
    "    all_folds = []\n",
    "    for i in range(k):\n",
    "        val_lines = folds[i]\n",
    "        train_lines = [line for j, f in enumerate(folds) if j != i for line in f]\n",
    "\n",
    "        train_df = lines_to_df(train_lines)\n",
    "        val_df = lines_to_df(val_lines)\n",
    "        all_folds.append((train_df, val_df))\n",
    "    \n",
    "    return all_folds\n",
    "\n",
    "def train_network(net, x_train, y_train, learning_rate):\n",
    "    \"\"\"\n",
    "    Trains the network for a single epoch, running the forward and backward pass, and compute and return the loss.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    z1, a1, z2, a2  = net.forward(x_train)\n",
    "    # backward pass\n",
    "    backward(net, x_train, y_train, z1, a1, z2, a2, learning_rate)\n",
    "    loss = np.mean((a2 - y_train) ** 2)\n",
    "    return loss\n",
    "        \n",
    "def eval_network(net, x_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Evaluates the network ; Note that we do not update weights (no backward pass)\n",
    "    \"\"\"\n",
    "    z1, a1, z2, a2 = net.forward(x_valid)\n",
    "    loss = np.mean((a2-y_valid)**2)\n",
    "    return loss\n",
    "\n",
    "# Model saving and loading functions\n",
    "def save_ffnn_model(filepath, model):\n",
    "    if not filepath.endswith('.pkl'):\n",
    "        filepath = filepath+'.pkl'\n",
    "    with open(filepath, 'wb') as f:\n",
    "        dict_to_save = {'input_size': model.W1.shape[0], 'hidden_size':model.W1.shape[1], 'output_size':model.W2.shape[1],\n",
    "                        'W1': model.W1, 'b1':model.b1, 'W2':model.W2, 'b2':model.b2}\n",
    "        pickle.dump(dict_to_save, f)\n",
    "        print(f'Saved FFNN model at {filepath}')\n",
    "\n",
    "\n",
    "def load_ffnn_model(filepath, model=None):\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "    if model is None:\n",
    "            model = SimpleFFNN(loaded_dict['input_size'], loaded_dict['hidden_size'], loaded_dict['output_size'])\n",
    "    assert (model.W1.shape[0]==loaded_dict['input_size'] and model.W1.shape[1]==loaded_dict['hidden_size'] and model.W2.shape[1]==loaded_dict['output_size']), \\\n",
    "        f\"Model and loaded weights size mismatch!. Provided model has weight of dimensions {model.W1.shape, model.W2.shape} ; Loaded weights have shape {loaded_dict['W1'].shape, loaded_dict['W2'].shape}\"\n",
    "\n",
    "    model.W1 = loaded_dict['W1']\n",
    "    model.b1 = loaded_dict['b1']\n",
    "    model.W2 = loaded_dict['W2']\n",
    "    model.b2 = loaded_dict['b2']\n",
    "    print(f\"Model loaded successfully from {filepath}\\nwith weights [ W1, W2 ] dimensions : {model.W1.shape, model.W2.shape}\")\n",
    "    return model\n",
    "\n",
    "def plot_losses(train_losses, valid_losses, n_epochs, title=None):\n",
    "    # Plotting the losses \n",
    "    fig,ax = plt.subplots(1,1, figsize=(9,5))\n",
    "    ax.plot(range(n_epochs), train_losses, label='Train loss', c='b')\n",
    "    ax.plot(range(n_epochs), valid_losses, label='Valid loss', c='m')\n",
    "    ax.legend()\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6dedd23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating data from ../data_prod/phage_minhash_12_combined/Pectobacterium_brasiliense_combined.txt into train/val and test sets...\n",
      "Total lines: 1132638, Train/Val lines: 1019374, Test lines: 113264\n",
      "Ratios - Train/Val: 0.90, Test: 0.10\n",
      "\n",
      "Training/Validation data saved to: ../data_prod/NN_files/Pectobacterium_brasiliense_train_val.txt\n",
      "Test data saved to: ../data_prod/NN_files/Pectobacterium_brasiliense_test.txt\n"
     ]
    }
   ],
   "source": [
    "train_val_path, test_path = separate_test(full_path, train_val_ratio=0.9, test_ratio=0.1, TS=True)\n",
    "print(f\"Training/Validation data saved to: {train_val_path}\")\n",
    "print(f\"Test data saved to: {test_path}\")\n",
    "\n",
    "#Retun list of paths were files start with Pecto in data_prod_path+f\"phage_minhash_{K}_combined/\n",
    "# combined_path = data_prod_path+f\"phage_minhash_{K}_combined/\"\n",
    "# pecto_files = [combined_path+path for path in os.listdir(combined_path) if path.startswith(\"Pecto\")]\n",
    "# print(pecto_files)\n",
    "# train_val_path, test_path = separate_test(pecto_files, train_val_ratio=0.9, test_ratio=0.1, TS=True)\n",
    "# print(f\"Training/Validation data saved to: {train_val_path}\")\n",
    "# print(f\"Test data saved to: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0f30a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train data length: 815499, Val data length: 203875\n",
      "Fold 2: Train data length: 815499, Val data length: 203875\n",
      "Fold 3: Train data length: 815499, Val data length: 203875\n",
      "Fold 4: Train data length: 815499, Val data length: 203875\n",
      "Fold 5: Train data length: 815500, Val data length: 203874\n"
     ]
    }
   ],
   "source": [
    "#Purely to define split, no writing\n",
    "k_fold_data = k_fold_split(train_val_path, k=5, seed=42)\n",
    "for fold_idx, (train_data, val_data) in enumerate(k_fold_data):\n",
    "    print(f\"Fold {fold_idx+1}: Train data length: {len(train_data)}, Val data length: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4108d08a",
   "metadata": {},
   "source": [
    "### Defining NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ae3a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights initialization function.\n",
    "def xavier_initialization_normal(input_dim, output_dim):\n",
    "    shape = (input_dim, output_dim)\n",
    "    stddev = np.sqrt(2 / (input_dim + output_dim))\n",
    "    return np.random.normal(0, stddev, size=shape) * 0.1\n",
    "\n",
    "def random_initialization_normal(input_dim, output_dim):\n",
    "    return np.random.randn(input_dim, output_dim) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b2627761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFFNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, initialization_function=xavier_initialization_normal):\n",
    "        # Initialize weights and biases with small random values\n",
    "        # initialization_function(input_dim, output_dim) -> np.array of shape (input_dim, output_dim)\n",
    "        self.W1 = initialization_function(input_size, hidden_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = initialization_function(hidden_size, output_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        print(f'Input -> Hidden Layer Weight Matrix Shape: {self.W1.shape}',\n",
    "              f'First Layer Bias Weights Vector Shape: {self.b1.shape}',\n",
    "              f'Hidden -> Output layer Weight Matrix Shape: {self.W2.shape}',\n",
    "              f'Second Layer Bias Weights Vector Shape: {self.b2.shape}', sep=\"\\n\")\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x): \n",
    "        \"\"\"\n",
    "        The normal version of sigmoid 1 / (1 + np.exp(-x)) is NOT numerically stable\n",
    "        Here we split the case into two for positive and negative inputs\n",
    "        because np.exp(-x) for something negative will quickly overflow if x is a large negative number\n",
    "        \"\"\"\n",
    "        # This is equivalent to : \n",
    "        # if x>=0, then compute (1/(1+np.exp(-x)))\n",
    "        # if x<0: compute (np.exp(x)/(1+np.exp(x))))\n",
    "        return np.where(x >= 0, 1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x:\n",
    "        zi denotes the output of a hidden layer i\n",
    "        ai denotes the output of an activation function (non-linearity) at layer i\n",
    "        (activations are relu, sigmoid, tanh, etc.)\n",
    "        Use self.function to call a method. for example: self.relu(XX)\n",
    "        \"\"\"\n",
    "\n",
    "        # First layer : Use a relu here for the activation \n",
    "        z1 = np.dot(x, self.W1) + self.b1 #XX\n",
    "        a1 = self.relu(z1) #XX\n",
    "        \n",
    "        # Output layer : Use a sigmoid here for the activation\n",
    "        z2 = np.dot(a1, self.W2) + self.b2 #XX\n",
    "        a2 = self.sigmoid(z2) #XX\n",
    "        \n",
    "        # Return all the intermediate outputs as well because we need them for backpropagation (see slides)\n",
    "        return z1, a1, z2, a2\n",
    "\n",
    "def relu_derivative(a):\n",
    "    return np.where(a > 0, 1, 0)\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    \"\"\"\n",
    "    For this derivative, it is not necessary to find a numerically stable version.\n",
    "    Just take the base formula and derive it.    \n",
    "    \"\"\"\n",
    "    return np.array(a*(1 - a))\n",
    "\n",
    "\n",
    "def backward(net, x, y, z1, a1, z2, a2, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Function to backpropagate the gradients from the output to update the weights.\n",
    "    Apply the chain rule and slowly work out the chain derivatives from the output back to the input\n",
    "    Reminder that np.dot(array_1, array_2) and array.T exists to transpose an array for matrix multiplication\n",
    "    \"\"\"\n",
    "    # This assumes that we are computing a MSE as the loss function.\n",
    "    # Look at your slides to compute the gradient backpropagation for a mean-squared error using the chain rule.\n",
    "\n",
    "    # Output layer error ; We used a sigmoid in this layer\n",
    "    dE_dO = a2 - y\n",
    "    dO_do = sigmoid_derivative(a2)\n",
    "    dE_do = dE_dO * dO_do\n",
    "\n",
    "    #print(\"dE_do\", dE_do.shape)\n",
    "    ### (REMEMBER for np.dot(A,B) columns of A MUST equal rows in B) ###\n",
    "    \n",
    "    # Backpropagate to hidden layer \n",
    "    #print(\"a1\", a1.shape)\n",
    "    dE_dW2 = np.dot(dE_do.T, a1)\n",
    "    dE_db2 = np.sum(dE_do, axis=0, keepdims=True)\n",
    "    dE_db2 = dE_db2.squeeze() # Squeeze is needed here to make the dimensions fit\n",
    "    #print(\"dE_dW2\", dE_dW2.shape)\n",
    "    #print(\"dE_db2\", dE_dW2.shape)\n",
    "\n",
    "    # Hidden layer error ; We used a ReLU in this layer!\n",
    "    # (O âˆ’ t)â‹… g'(o)â‹…wj\n",
    "    dE_dH = np.dot(dE_do, net.W2.T)\n",
    "    #print(\"dE_dH\", dE_dH.shape)\n",
    "    dE_dh = dE_dH * relu_derivative(a1)\n",
    "    #print(\"dE_dh\", dE_dh.shape)\n",
    "\n",
    "    # Backpropagate to input layer\n",
    "    dE_dW1 = np.dot(dE_dh.T, x)\n",
    "    #print(\"dE_dh\", dE_dh.shape)\n",
    "    dE_db1 = np.sum(dE_dh, axis=0, keepdims=True) \n",
    "    dE_db1 = dE_db1.squeeze() # Squeeze is needed here to make the dimensions fit\n",
    "    #print(\"dE_db1\", dE_db1.shape)\n",
    "\n",
    "    # Update weights and biases using gradient descent\n",
    "    net.W1 -= learning_rate * dE_dW1.T\n",
    "    #print(\"W1 shapes:\", net.W1.shape, dE_dW1.T.shape)\n",
    "    net.b1 -= learning_rate * dE_db1.T\n",
    "    #print(\"b1 shapes:\", net.b1.shape, dE_db1.T.shape)\n",
    "    net.W2 -= learning_rate * dE_dW2.T\n",
    "    #print(\"W2 shapes:\", net.W2.shape, dE_dW2.T.shape)\n",
    "    net.b2 -= learning_rate * dE_db2.T\n",
    "    #print(\"b2 shapes:\", net.b2.shape, dE_db2.T.shape)\n",
    "\n",
    "def encode_new_data(X_in):\n",
    "    \"\"\"\n",
    "    Encode the large integer feature using log transformation and normalization.\n",
    "    Returns a tuple: (X_out, Y_out)\n",
    "      X_out: tensor of shape (batch_size, 1)\n",
    "      Y_out: tensor of shape (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Extract the large integer feature (assuming it's the first column)\n",
    "    # Convert to float for safe log calculation\n",
    "    feature_column = X_in.iloc[:, 0].astype(float)\n",
    "    \n",
    "    # 2. Log Transformation\n",
    "    # Use log10 or natural log (np.log) - log10 is often easier to interpret\n",
    "    # Add a small epsilon or check for zero if your data could contain 0,\n",
    "    # but based on your example, it's safe to use log10.\n",
    "    log_transformed = np.log10(feature_column)\n",
    "    \n",
    "    # 3. Simple Normalization (Min-Max or Z-score)\n",
    "    # Using Min-Max scaling for this example: scale to [0, 1]\n",
    "    min_val = log_transformed.min()\n",
    "    max_val = log_transformed.max()\n",
    "    \n",
    "    # Handle case where min == max to avoid division by zero (e.g., if batch_size=1)\n",
    "    if max_val == min_val:\n",
    "        normalized_feature = np.zeros_like(log_transformed)\n",
    "    else:\n",
    "        normalized_feature = (log_transformed - min_val) / (max_val - min_val)\n",
    "        \n",
    "    # 4. Prepare X_out and Y_out\n",
    "    # X_out shape: (batch_size, 1) - One feature per sample\n",
    "    X_out = np.expand_dims(normalized_feature.values, 1)\n",
    "    \n",
    "    # Y_out shape: (batch_size, 1) - Target values\n",
    "    # Assuming the target is the second column (0 or 1)\n",
    "    Y_out = np.expand_dims(X_in.iloc[:, 1].values, 1)\n",
    "            \n",
    "    return X_out.astype(np.float32), Y_out.astype(np.int8)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f5942",
   "metadata": {},
   "source": [
    "### Train & Validate model\n",
    "an encoder function is needed to parse my train and val data, in order to perform 2 crucial steps:\n",
    "1. Fixing Sequence Length (Padding and Truncation)\n",
    "2. Creating the 3D Tensor Structure; $$(\\text{Batch Size}, \\text{Sequence Length}, \\text{Feature Dimensions})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea4990",
   "metadata": {},
   "source": [
    "#### Encode data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "80e9fdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train data length: 815499, Val data length: 203875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>385582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>429917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "label        \n",
       "0      385582\n",
       "1      429917"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(815499, 1)\n",
      "[[1.1929211e+19]\n",
      " [5.7899799e+18]\n",
      " [1.5367572e+19]\n",
      " ...\n",
      " [5.1122508e+18]\n",
      " [3.3897658e+18]\n",
      " [8.8569400e+18]]\n",
      "Input -> Hidden Layer Weight Matrix Shape: (1, 50)\n",
      "First Layer Bias Weights Vector Shape: (50,)\n",
      "Hidden -> Output layer Weight Matrix Shape: (50, 1)\n",
      "Second Layer Bias Weights Vector Shape: (1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sd/ygh_pqf56ygdrztjkzy96_s00000gn/T/ipykernel_2641/2349245942.py:26: RuntimeWarning: overflow encountered in exp\n",
      "  return np.where(x >= 0, 1 / (1 + np.exp(-x)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.5272\tValid Loss: 0.5271\n",
      "Epoch 25: Train Loss: 0.5272\tValid Loss: 0.5271\n",
      "Epoch 50: Train Loss: 0.5272\tValid Loss: 0.5271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Run n_epochs of training\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 32\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m eval_network(network, x_valid_, y_valid_)\n\u001b[1;32m     34\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[84], line 94\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(net, x_train, y_train, learning_rate)\u001b[0m\n\u001b[1;32m     92\u001b[0m z1, a1, z2, a2  \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(x_train)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((a2 \u001b[38;5;241m-\u001b[39m y_train) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[90], line 87\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(net, x, y, z1, a1, z2, a2, learning_rate)\u001b[0m\n\u001b[1;32m     81\u001b[0m dE_db2 \u001b[38;5;241m=\u001b[39m dE_db2\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;66;03m# Squeeze is needed here to make the dimensions fit\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#print(\"dE_dW2\", dE_dW2.shape)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#print(\"dE_db2\", dE_dW2.shape)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Hidden layer error ; We used a ReLU in this layer!\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# (O âˆ’ t)â‹… g'(o)â‹…wj\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m dE_dH \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdE_do\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#print(\"dE_dH\", dE_dH.shape)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m dE_dh \u001b[38;5;241m=\u001b[39m dE_dH \u001b[38;5;241m*\u001b[39m relu_derivative(a1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Purely to define split, no writing\n",
    "k_fold_data = k_fold_split(train_val_path, k=5, seed=42)\n",
    "for fold_idx, (train_data, val_data) in enumerate(k_fold_data):\n",
    "    print(f\"Fold {fold_idx+1}: Train data length: {len(train_data)}, Val data length: {len(val_data)}\")\n",
    "    display(train_data.groupby(\"label\").agg(count=('kmer_hash','count')))\n",
    "\n",
    "    #Encoding data\n",
    "    x_train_, y_train_ = encode_new_data(train_data)\n",
    "    x_valid_, y_valid_ = encode_new_data(val_data)\n",
    "    print(x_train_.shape)\n",
    "    print(x_train_)\n",
    "\n",
    "    #Initializing model\n",
    "    input_size = x_train_.shape[1] # also known as \"n_features\"\n",
    "    # Model and training hyperparameters\n",
    "    learning_rate = 0.0001\n",
    "    hidden_units = 50\n",
    "    n_epochs = 500\n",
    "    output_size = 1\n",
    "    # Creating a model instance \n",
    "    # You can use either `xavier_initialization_normal` or `random_initialization_normal`\n",
    "    # for the initialization_function argument of the class\n",
    "    network = SimpleFFNN(input_size, hidden_units, output_size)#, \n",
    "                        #initialization_function=xavier_initialization_normal)\n",
    "\n",
    "    # Training loops\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # Run n_epochs of training\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_network(network, x_train_, y_train_, learning_rate)\n",
    "        valid_loss = eval_network(network, x_valid_, y_valid_)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        # For the first, every 5% of the epochs and last epoch, we print the loss \n",
    "        # to check that the model is properly training. (loss going down)\n",
    "        if (n_epochs >= 10 and epoch % math.ceil(0.05 * n_epochs) == 0) or epoch == 0 or epoch == n_epochs:\n",
    "            print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}\\tValid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    # save model + plot losses (put your own savename to be used for the model and predictions)\n",
    "    model_savepath = NN_files_path+'some_ffnn_model.pkl' # /path/to/your/stuff/filename.pkl\n",
    "    save_ffnn_model(model_savepath, model=network)\n",
    "\n",
    "    # plotting the losses \n",
    "    plot_losses(train_losses, valid_losses, n_epochs, title=f\"Train & Val loss for {selected_bact_species} with {K}mers\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5490920d",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00772ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bfa10d6",
   "metadata": {},
   "source": [
    "# Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "K = 18\n",
    "data_prod_path = \"../data_prod/\"\n",
    "all_phage_minhash_txt_path = data_prod_path + f\"phage_minhash_{K}_combined/\"\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(all_phage_minhash_txt_path):\n",
    "    if filename.endswith('_combined.txt'):\n",
    "        filepath = os.path.join(all_phage_minhash_txt_path, filename)\n",
    "        \n",
    "        # Read the file into a dataframe\n",
    "        df = pd.read_csv(filepath, sep='\\t', header=None, names=['kmer_hash', 'label'])\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df.to_csv(all_phage_minhash_txt_path+\"../phage_18_all.txt\", sep=\"\\t\")\n",
    "\n",
    "print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combining files that starts with Pectobacterium\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "K = 18\n",
    "data_prod_path = \"../data_prod/\"\n",
    "all_phage_minhash_txt_path = data_prod_path + f\"phage_minhash_{K}_combined/\"\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(all_phage_minhash_txt_path):\n",
    "    if filename.startswith('Pecto'):\n",
    "        filepath = os.path.join(all_phage_minhash_txt_path, filename)\n",
    "        \n",
    "        # Read the file into a dataframe\n",
    "        df = pd.read_csv(filepath, sep='\\t', header=None, names=['kmer_hash', 'label'])\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "pecto_df = pd.concat(dfs, ignore_index=True)\n",
    "pecto_df.to_csv(all_phage_minhash_txt_path+\"../phage_18_pecto.txt\", sep=\"\\t\")\n",
    "\n",
    "print(f\"pecto_df dataframe shape: {pecto_df.shape}\")\n",
    "print(pecto_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec391f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "K = 18\n",
    "selected_bact_species = \"Pectobacterium brasiliense\"\n",
    "data_prod_path = \"../data_prod/\"\n",
    "full_path = data_prod_path+f\"phage_minhash_{K}_combined/{selected_bact_species.replace(' ', '_')}_combined.txt\"\n",
    "# --- 1. Raw Data Input ---\n",
    "# The provided data of minhashed k-mers and their binary labels (0/1).\n",
    "data_str = \"\"\"\n",
    "14104189032391025891\t0\n",
    "9272555873177485702\t1\n",
    "14099220935155194787\t0\n",
    "14904868568718467447\t0\n",
    "4598318341245526010\t0\n",
    "12928218682532402448\t0\n",
    "17766489500413828799\t0\n",
    "12262843827502767463\t1\n",
    "15191645544529172491\t0\n",
    "10259680472464355762\t1\n",
    "18329537351721549328\t0\n",
    "1168172853346433374\t1\n",
    "2081507574639483341\t1\n",
    "15806857621905048525\t1\n",
    "13505998584889076293\t1\n",
    "683483166365466956\t0\n",
    "\"\"\"\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "#df = pd.read_csv(full_path, sep='\\t', header=None, names=['kmer_hash', 'label'])\n",
    "#df = combined_df\n",
    "df = pecto_df\n",
    "\n",
    "# Separate the single numerical feature (X) and the binary labels (y)\n",
    "X = df[['kmer_hash']].values\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 2. Data Splitting (60/20/20 for Train/Validation/Test) ---\n",
    "\n",
    "# Step 1: Split into Training (50%) and Temporary (50%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.5,  \n",
    "    random_state=42, \n",
    "    shuffle=True, \n",
    "    stratify=y       # Important for small datasets to keep class balance\n",
    ")\n",
    "\n",
    "# Step 2: Split Temporary (50%) into Validation (25%) and Testing (25%) sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Split completed: Train={len(X_train)}, Validation={len(X_val)}, Test={len(X_test)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 3. Preprocessing: Feature Scaling (MANDATORY for large integers) ---\n",
    "# We use StandardScaler to normalize the huge hash values to have zero mean and unit variance.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data to prevent data leakage\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Transform the validation and test sets using the training data's parameters\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature Scaling complete. Data is ready for the network.\")\n",
    "# Example of scaled values (will be close to zero)\n",
    "print(f\"Example scaled k-mer value: {X_train_scaled[0][0]:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 4. Define the Neural Network Model ---\n",
    "# A simple feedforward network for binary classification.\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = Sequential([\n",
    "        # Input layer expects a single feature (the scaled hash value)\n",
    "        Dense(32, activation='relu', input_shape=(1,), name='Input_Layer'), \n",
    "        # Hidden layer 1\n",
    "        Dense(16, activation='relu', name='Hidden_Layer_1'),\n",
    "        # Output layer for binary classification: 1 unit with sigmoid activation\n",
    "        Dense(1, activation='sigmoid', name='Output_Layer') \n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy', # Standard loss for binary output\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_classifier_model()\n",
    "model.summary()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 5. Train and Evaluate ---\n",
    "\n",
    "print(\"Starting model training (100 Epochs)...\")\n",
    "\n",
    "model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    epochs=2,           \n",
    "    batch_size=4,         \n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    verbose=1             # Keep the output clean\n",
    ")\n",
    "\n",
    "# Evaluate on the final, unseen test set\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"Training Complete.\")\n",
    "print(f\"Test Accuracy on unseen data: {accuracy:.4f}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. ROC Curve Analysis and Plotting (NEW SECTION) ---\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "\n",
    "# Predict probabilities on the test set\n",
    "# .ravel() converts the (n, 1) prediction array to a (n,) vector\n",
    "y_pred_proba = model.predict(X_test_scaled).ravel()\n",
    "\n",
    "# Calculate ROC curve metrics (False Positive Rate, True Positive Rate, and Thresholds)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Calculate the Area Under the Curve (AUC)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Plot the ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Random Classifier (AUC = 0.50)')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.savefig(\"ROC_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921078d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1de4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
